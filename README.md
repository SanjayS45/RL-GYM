# RL-GYM ğŸ‹ï¸â€â™‚ï¸

**Interactive Reinforcement Learning Training Platform**

*Inspired by AI Warehouse YouTube Channel*

<p align="center">
  <img src="https://img.shields.io/badge/Version-1.0.0-brightgreen.svg" alt="Version">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/React-18+-61dafb.svg" alt="React">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

---

## ğŸ¯ Overview

RL-GYM is a complete, interactive platform for training reinforcement learning agents. Watch agents learn in real-time, modify hyperparameters on the fly, and define custom environments with natural language goals.

### Key Features

- ğŸ® **Real-time Visualization**: Watch your agents learn and improve
- ğŸ§  **Multiple RL Algorithms**: DQN, PPO, SAC, A2C out of the box
- ğŸŒ **Custom Environments**: GridWorld, Navigation, Platformer
- ğŸ’¬ **Natural Language Goals**: Define objectives in plain English
- ğŸ“Š **Live Metrics**: Track training progress with beautiful charts
- ğŸ›ï¸ **Interactive Controls**: Modify parameters during training
- ğŸ“ **Dataset Support**: Import demonstrations and offline data

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Frontend                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Environ â”‚  â”‚  Agent  â”‚  â”‚Training â”‚  â”‚ Visual  â”‚  â”‚Metrics â”‚â”‚
â”‚  â”‚  Setup  â”‚  â”‚ Params  â”‚  â”‚Controls â”‚  â”‚  izer   â”‚  â”‚  Panel â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                 â”‚                               â”‚
â”‚                        WebSocket + REST API                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Backend                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   RL Core   â”‚  â”‚      Training Manager     â”‚  â”‚  Datasets  â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚â”‚
â”‚  â”‚  â”‚  DQN  â”‚  â”‚  â”‚  â”‚Session â”‚  â”‚Callbacksâ”‚  â”‚  â”‚  â”‚Loaderâ”‚  â”‚â”‚
â”‚  â”‚  â”‚  PPO  â”‚  â”‚  â”‚  â”‚ Managerâ”‚  â”‚         â”‚  â”‚  â”‚  â”‚Valid.â”‚  â”‚â”‚
â”‚  â”‚  â”‚  SAC  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚
â”‚  â”‚  â”‚  A2C  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                 â”‚            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                   â”‚      Environments          â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚
â”‚  â”‚     NLP     â”‚  â”‚  â”‚GridWorldâ”‚  â”‚ Navig.  â”‚ â”‚               â”‚
â”‚  â”‚ Goal Parser â”‚  â”‚  â”‚Platform â”‚  â”‚ Physics â”‚ â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- npm or yarn

### Installation

```bash
# Clone the repository
git clone https://github.com/SanjayS45/RL-GYM.git
cd RL-GYM

# Install all dependencies
make install
```

### Running the Application

```bash
# Start both backend and frontend (recommended)
make dev

# Or start them separately:
make backend  # http://localhost:8000
make frontend # http://localhost:5173
```

### Using Docker

```bash
# Start with Docker Compose
docker-compose up -d

# View logs
docker-compose logs -f
```

---

## ğŸ“¦ Project Structure

```
RL-GYM/
â”œâ”€â”€ backend/                 # Python backend
â”‚   â”œâ”€â”€ rl_core/            # RL algorithms and utilities
â”‚   â”‚   â”œâ”€â”€ algorithms/     # DQN, PPO, SAC, A2C
â”‚   â”‚   â”œâ”€â”€ base.py         # Base policy class
â”‚   â”‚   â”œâ”€â”€ networks.py     # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ buffers.py      # Replay buffer
â”‚   â”‚   â””â”€â”€ utils.py        # Utilities
â”‚   â”œâ”€â”€ environments/       # Custom Gym environments
â”‚   â”‚   â”œâ”€â”€ grid_world.py   # Discrete navigation
â”‚   â”‚   â”œâ”€â”€ navigation.py   # Continuous navigation
â”‚   â”‚   â””â”€â”€ platformer.py   # 2D platformer
â”‚   â”œâ”€â”€ nlp/                # Natural language processing
â”‚   â”‚   â”œâ”€â”€ goal_parser.py  # Parse NL goals
â”‚   â”‚   â””â”€â”€ reward_generator.py
â”‚   â”œâ”€â”€ datasets/           # Dataset management
â”‚   â”œâ”€â”€ training/           # Training orchestration
â”‚   â”œâ”€â”€ api/                # FastAPI server
â”‚   â””â”€â”€ config.py           # Configuration
â”œâ”€â”€ frontend/               # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # UI components
â”‚   â”‚   â”œâ”€â”€ hooks/          # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ store/          # Zustand state
â”‚   â”‚   â””â”€â”€ App.tsx         # Main app
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ examples/               # Example configurations
â”œâ”€â”€ docker-compose.yml      # Docker setup
â”œâ”€â”€ Makefile               # Development commands
â””â”€â”€ README.md
```

---

## ğŸ® Usage

### 1. Environment Setup

Choose from predefined environments:
- **GridWorld**: Discrete navigation on a grid
- **Navigation**: Continuous 2D navigation with lidar
- **Platformer**: 2D platformer with jumping

Or customize with obstacles and goals!

### 2. Agent Configuration

Select an algorithm and tune hyperparameters:

| Algorithm | Best For | Key Parameters |
|-----------|----------|----------------|
| **DQN** | Discrete actions | Îµ-greedy, replay buffer |
| **PPO** | Both action types | clip range, GAE Î» |
| **SAC** | Continuous actions | entropy Î±, soft updates |
| **A2C** | Fast training | n-steps, value coefficient |

### 3. Natural Language Goals

Define goals in plain English:

```
"Reach the green target while avoiding red obstacles"
"Navigate to the goal using the shortest path"
"Collect all coins without falling off platforms"
```

### 4. Training

Start training and watch your agent learn:
- Real-time visualization
- Live metrics (reward, loss, episode length)
- Pause/resume/stop controls
- Speed adjustment

---

## ğŸ”§ API Reference

### REST Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/training/start` | POST | Start training session |
| `/training/stop/{id}` | POST | Stop training |
| `/training/status/{id}` | GET | Get session status |
| `/environments/list` | GET | List available environments |
| `/agents/algorithms` | GET | List available algorithms |
| `/datasets/upload` | POST | Upload dataset |

### WebSocket

Connect to `/training/ws` for real-time updates:

```javascript
const ws = new WebSocket('ws://localhost:8000/training/ws');
ws.onmessage = (event) => {
  const update = JSON.parse(event.data);
  // Handle training update
};
```

---

## ğŸ“Š Example Configurations

See the `examples/` directory for ready-to-use configurations:

- `gridworld_dqn.json` - DQN on GridWorld
- `navigation_ppo.json` - PPO on Navigation
- `platformer_sac.json` - SAC on Platformer
- `natural_language_goal.json` - Using NL goals

---

## ğŸ§ª Testing

```bash
# Run backend tests
make test

# Run specific component tests
cd backend && python test_components.py
```

---

## ğŸ› ï¸ Development

### Commands

```bash
make install         # Install dependencies
make dev             # Start development servers
make backend         # Start backend only
make frontend        # Start frontend only
make test            # Run tests
make build           # Build for production
make clean           # Clean build artifacts
```

### Adding a New Algorithm

1. Create a new file in `backend/rl_core/algorithms/`
2. Extend `BasePolicy` class
3. Implement `act()`, `learn()`, `save()`, `load()`
4. Export from `__init__.py`
5. Add to API routes

### Adding a New Environment

1. Create a new file in `backend/environments/`
2. Extend `gymnasium.Env`
3. Implement `reset()`, `step()`, `render()`
4. Export from `__init__.py`
5. Add to API routes

---

## ğŸ“ˆ Roadmap

- [ ] Multi-agent support
- [ ] Curriculum learning
- [ ] Model-based RL algorithms
- [ ] Custom reward shaping UI
- [ ] Training comparison tools
- [ ] Cloud deployment options

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Inspired by [AI Warehouse](https://www.youtube.com/@AIWarehouse) YouTube channel
- Built with [PyTorch](https://pytorch.org/), [FastAPI](https://fastapi.tiangolo.com/), [React](https://react.dev/)
- Environment design influenced by [Gymnasium](https://gymnasium.farama.org/)

---

<p align="center">
  Made with â¤ï¸ for the RL community
</p>
